{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch with Examples\n",
    "\n",
    "This is an introduction to Pytorch following the examples according to:\n",
    "\n",
    " - [1] http://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T02:52:27.567024Z",
     "start_time": "2018-02-12T02:52:23.969786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27091220.0\n",
      "100 263.90582275390625\n",
      "200 0.7395830750465393\n",
      "300 0.0033662114292383194\n",
      "400 0.00012202889047330245\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # This runs on GPU\n",
    "\n",
    "# batch size (N), input dim (D_in), output dim (D_out), hidden dim (H)\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "# Randomly initialize weight matrices\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward pass\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    # Compute and predict loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if(t % 100 == 0):\n",
    "        print(t, loss)\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch: Variables and AutoGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:07:26.976496Z",
     "start_time": "2018-02-12T03:07:26.479459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26133316.0\n",
      "100 454.1337585449219\n",
      "200 1.5466086864471436\n",
      "300 0.008519974537193775\n",
      "400 0.00019258004613220692\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor\n",
    "\n",
    "# batch size (N), input dim (D_in), output dim (D_out), hidden dim (H)\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create input and output tensors, but wrap them in variables\n",
    "# The requires_grad field denotes that gradients do not need \n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Weights, you do want gradients for these variables\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass (you can chain operations since you dont need intermediate values)\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    # Loss is now a variable, so to print it you need loss.data\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if(t % 100 == 0):\n",
    "        print(t, loss.data[0])\n",
    "    # Autograd will compute the backward pass\n",
    "    loss.backward()\n",
    "    # Update weights using gradient descent\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:16:37.608193Z",
     "start_time": "2018-02-12T03:16:37.006747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22577980.0\n",
      "100 509.048095703125\n",
      "200 4.342343807220459\n",
      "300 0.06760153919458389\n",
      "400 0.001541274250485003\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Subclass autograd.Function to create our own custom autograd functions.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors # Bring back saved tensor from forward pass\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor\n",
    "\n",
    "# batch size (N), input dim (D_in), output dim (D_out), hidden dim (H)\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Apply our function\n",
    "    relu = ReLU.apply\n",
    "    # forward pass with our custom autograd function\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if(t % 100 == 0):\n",
    "        print(t, loss.data[0])\n",
    "    # Backwards pass using autograd\n",
    "    loss.backward()\n",
    "    # Update weights using gradient descent\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T03:26:02.847980Z",
     "start_time": "2018-02-12T03:26:02.275437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 667.1468505859375\n",
      "100 1.7733511924743652\n",
      "200 0.02600976452231407\n",
      "300 0.0008200249867513776\n",
      "400 3.693465987453237e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn module to define our model in a sequential manner\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n",
    "                           torch.nn.ReLU(),\n",
    "                           torch.nn.Linear(H, D_out))\n",
    "\n",
    "# The nn module also contains loss functions\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # model can be called as a variable. It outputs a variable\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if(t % 100 == 0):\n",
    "        print(t, loss.data[0])\n",
    "    # Zero gradients before running backwards pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T04:06:19.292391Z",
     "start_time": "2018-02-12T04:06:17.914638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 731.6021728515625\n",
      "100 72.0875473022461\n",
      "200 1.479520559310913\n",
      "300 0.0060690464451909065\n",
      "400 1.0713240953919012e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# Use optim module to define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if(t % 100 == 0):\n",
    "        print(t, loss.data[0])\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom nn modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T04:18:48.310598Z",
     "start_time": "2018-02-12T04:18:47.446077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 657.40576171875\n",
      "1 605.4139404296875\n",
      "2 561.5408935546875\n",
      "3 523.2501831054688\n",
      "4 489.2220153808594\n",
      "5 458.61395263671875\n",
      "6 431.0629577636719\n",
      "7 405.8373107910156\n",
      "8 382.69390869140625\n",
      "9 360.9817810058594\n",
      "10 340.64019775390625\n",
      "11 321.7035827636719\n",
      "12 303.9287109375\n",
      "13 287.1660461425781\n",
      "14 271.30126953125\n",
      "15 256.2503967285156\n",
      "16 241.9177703857422\n",
      "17 228.29176330566406\n",
      "18 215.3031005859375\n",
      "19 202.99143981933594\n",
      "20 191.25765991210938\n",
      "21 180.08087158203125\n",
      "22 169.4584197998047\n",
      "23 159.38400268554688\n",
      "24 149.83360290527344\n",
      "25 140.7922821044922\n",
      "26 132.22720336914062\n",
      "27 124.13607025146484\n",
      "28 116.47787475585938\n",
      "29 109.2695083618164\n",
      "30 102.4994125366211\n",
      "31 96.10252380371094\n",
      "32 90.09012603759766\n",
      "33 84.44229125976562\n",
      "34 79.1405258178711\n",
      "35 74.1895980834961\n",
      "36 69.55003356933594\n",
      "37 65.20470428466797\n",
      "38 61.1375617980957\n",
      "39 57.33534622192383\n",
      "40 53.77180862426758\n",
      "41 50.43370056152344\n",
      "42 47.31364822387695\n",
      "43 44.403785705566406\n",
      "44 41.68169021606445\n",
      "45 39.14221954345703\n",
      "46 36.76384353637695\n",
      "47 34.53904342651367\n",
      "48 32.46078109741211\n",
      "49 30.522722244262695\n",
      "50 28.71068000793457\n",
      "51 27.01681900024414\n",
      "52 25.43260383605957\n",
      "53 23.950380325317383\n",
      "54 22.562040328979492\n",
      "55 21.26034164428711\n",
      "56 20.03953742980957\n",
      "57 18.895185470581055\n",
      "58 17.823375701904297\n",
      "59 16.81911849975586\n",
      "60 15.876144409179688\n",
      "61 14.991643905639648\n",
      "62 14.163412094116211\n",
      "63 13.383689880371094\n",
      "64 12.652059555053711\n",
      "65 11.965036392211914\n",
      "66 11.318665504455566\n",
      "67 10.71005630493164\n",
      "68 10.137306213378906\n",
      "69 9.598276138305664\n",
      "70 9.0911283493042\n",
      "71 8.613175392150879\n",
      "72 8.162940979003906\n",
      "73 7.7389678955078125\n",
      "74 7.338923931121826\n",
      "75 6.961512565612793\n",
      "76 6.606218338012695\n",
      "77 6.271040916442871\n",
      "78 5.954178810119629\n",
      "79 5.655114650726318\n",
      "80 5.372372627258301\n",
      "81 5.1050615310668945\n",
      "82 4.852934837341309\n",
      "83 4.614200592041016\n",
      "84 4.388176918029785\n",
      "85 4.174081325531006\n",
      "86 3.9713592529296875\n",
      "87 3.779461145401001\n",
      "88 3.5977776050567627\n",
      "89 3.425601005554199\n",
      "90 3.2626805305480957\n",
      "91 3.1081409454345703\n",
      "92 2.9615566730499268\n",
      "93 2.822277784347534\n",
      "94 2.690187454223633\n",
      "95 2.564695358276367\n",
      "96 2.445723056793213\n",
      "97 2.332789182662964\n",
      "98 2.225632905960083\n",
      "99 2.123770236968994\n",
      "100 2.0267205238342285\n",
      "101 1.9346503019332886\n",
      "102 1.8472418785095215\n",
      "103 1.7640639543533325\n",
      "104 1.6847912073135376\n",
      "105 1.609381914138794\n",
      "106 1.5376322269439697\n",
      "107 1.469376802444458\n",
      "108 1.40446138381958\n",
      "109 1.3425949811935425\n",
      "110 1.2835943698883057\n",
      "111 1.2274397611618042\n",
      "112 1.1739228963851929\n",
      "113 1.122860074043274\n",
      "114 1.0741393566131592\n",
      "115 1.0276236534118652\n",
      "116 0.9832552075386047\n",
      "117 0.9409881830215454\n",
      "118 0.9007026553153992\n",
      "119 0.8622369766235352\n",
      "120 0.8255342841148376\n",
      "121 0.7905153036117554\n",
      "122 0.7571180462837219\n",
      "123 0.7252321243286133\n",
      "124 0.6948103308677673\n",
      "125 0.6657301187515259\n",
      "126 0.6379722952842712\n",
      "127 0.6114491820335388\n",
      "128 0.5860958695411682\n",
      "129 0.5618916153907776\n",
      "130 0.5387858748435974\n",
      "131 0.5166712999343872\n",
      "132 0.49551695585250854\n",
      "133 0.47524672746658325\n",
      "134 0.45587706565856934\n",
      "135 0.4373193681240082\n",
      "136 0.4195612668991089\n",
      "137 0.40257522463798523\n",
      "138 0.3863104581832886\n",
      "139 0.37076425552368164\n",
      "140 0.35588961839675903\n",
      "141 0.3416276276111603\n",
      "142 0.3279503285884857\n",
      "143 0.3148535192012787\n",
      "144 0.3023175299167633\n",
      "145 0.2903096377849579\n",
      "146 0.27881380915641785\n",
      "147 0.2677823007106781\n",
      "148 0.257213830947876\n",
      "149 0.24707229435443878\n",
      "150 0.23735864460468292\n",
      "151 0.22805121541023254\n",
      "152 0.2191147357225418\n",
      "153 0.21055208146572113\n",
      "154 0.20233704149723053\n",
      "155 0.19446419179439545\n",
      "156 0.18691128492355347\n",
      "157 0.1796729415655136\n",
      "158 0.172724187374115\n",
      "159 0.1660608947277069\n",
      "160 0.15966340899467468\n",
      "161 0.15352657437324524\n",
      "162 0.14764264225959778\n",
      "163 0.1419805884361267\n",
      "164 0.13655157387256622\n",
      "165 0.13134197890758514\n",
      "166 0.12633757293224335\n",
      "167 0.12153149396181107\n",
      "168 0.11691642552614212\n",
      "169 0.11248758435249329\n",
      "170 0.1082330048084259\n",
      "171 0.10414540767669678\n",
      "172 0.10022302716970444\n",
      "173 0.09645358473062515\n",
      "174 0.0928366482257843\n",
      "175 0.08935629576444626\n",
      "176 0.08601104468107224\n",
      "177 0.08279699832201004\n",
      "178 0.07970669865608215\n",
      "179 0.07674030214548111\n",
      "180 0.07388956099748611\n",
      "181 0.07114730775356293\n",
      "182 0.06851072609424591\n",
      "183 0.06597725301980972\n",
      "184 0.06354200094938278\n",
      "185 0.06119781360030174\n",
      "186 0.058944910764694214\n",
      "187 0.05677548795938492\n",
      "188 0.054689306765794754\n",
      "189 0.05268361046910286\n",
      "190 0.050754789263010025\n",
      "191 0.048898737877607346\n",
      "192 0.047111716121435165\n",
      "193 0.04539372771978378\n",
      "194 0.04374144971370697\n",
      "195 0.04215000197291374\n",
      "196 0.04061806946992874\n",
      "197 0.03914480656385422\n",
      "198 0.03772781044244766\n",
      "199 0.036365266889333725\n",
      "200 0.03505324572324753\n",
      "201 0.033789388835430145\n",
      "202 0.032573215663433075\n",
      "203 0.03140273317694664\n",
      "204 0.030276289209723473\n",
      "205 0.029191697016358376\n",
      "206 0.028147049248218536\n",
      "207 0.027141394093632698\n",
      "208 0.02617359720170498\n",
      "209 0.025240551680326462\n",
      "210 0.02434241771697998\n",
      "211 0.02347698248922825\n",
      "212 0.02264494076371193\n",
      "213 0.02184230461716652\n",
      "214 0.021068915724754333\n",
      "215 0.020323870703577995\n",
      "216 0.01960676908493042\n",
      "217 0.018915647640824318\n",
      "218 0.018250154331326485\n",
      "219 0.017608240246772766\n",
      "220 0.016989707946777344\n",
      "221 0.01639413833618164\n",
      "222 0.015819936990737915\n",
      "223 0.015266182832419872\n",
      "224 0.014733238145709038\n",
      "225 0.014219122938811779\n",
      "226 0.01372419111430645\n",
      "227 0.013246905989944935\n",
      "228 0.012786583974957466\n",
      "229 0.012342562898993492\n",
      "230 0.011914452537894249\n",
      "231 0.011502097360789776\n",
      "232 0.011104417033493519\n",
      "233 0.01072068139910698\n",
      "234 0.010350564494729042\n",
      "235 0.00999373197555542\n",
      "236 0.00964955985546112\n",
      "237 0.00931776873767376\n",
      "238 0.008997672237455845\n",
      "239 0.008689140900969505\n",
      "240 0.008391406387090683\n",
      "241 0.00810442678630352\n",
      "242 0.00782736111432314\n",
      "243 0.007559849880635738\n",
      "244 0.007301999256014824\n",
      "245 0.007053233217447996\n",
      "246 0.00681354058906436\n",
      "247 0.006582232657819986\n",
      "248 0.0063586425967514515\n",
      "249 0.006142876576632261\n",
      "250 0.005934819113463163\n",
      "251 0.005733948200941086\n",
      "252 0.005540010053664446\n",
      "253 0.005352864507585764\n",
      "254 0.005172385834157467\n",
      "255 0.004998236894607544\n",
      "256 0.004830178804695606\n",
      "257 0.004667987581342459\n",
      "258 0.004511108621954918\n",
      "259 0.004359759856015444\n",
      "260 0.004213676787912846\n",
      "261 0.004072727635502815\n",
      "262 0.0039366791024804115\n",
      "263 0.0038051593583077192\n",
      "264 0.0036781125236302614\n",
      "265 0.0035554892383515835\n",
      "266 0.0034370755311101675\n",
      "267 0.0033227175008505583\n",
      "268 0.00321221468038857\n",
      "269 0.0031056159641593695\n",
      "270 0.0030026256572455168\n",
      "271 0.0029031389858573675\n",
      "272 0.002807120094075799\n",
      "273 0.002714363858103752\n",
      "274 0.002624825108796358\n",
      "275 0.002538241446018219\n",
      "276 0.002454617293551564\n",
      "277 0.0023738814052194357\n",
      "278 0.0022957646287977695\n",
      "279 0.0022203437983989716\n",
      "280 0.0021474603563547134\n",
      "281 0.0020770239643752575\n",
      "282 0.0020089976023882627\n",
      "283 0.0019432881381362677\n",
      "284 0.0018797045340761542\n",
      "285 0.0018182810163125396\n",
      "286 0.0017589019844308496\n",
      "287 0.0017015134217217565\n",
      "288 0.0016460660845041275\n",
      "289 0.0015924667241051793\n",
      "290 0.0015407054452225566\n",
      "291 0.0014907270669937134\n",
      "292 0.0014423459069803357\n",
      "293 0.0013955471804365516\n",
      "294 0.0013503500958904624\n",
      "295 0.0013066251995041966\n",
      "296 0.0012643409427255392\n",
      "297 0.0012235167669132352\n",
      "298 0.0011840147199109197\n",
      "299 0.0011458112858235836\n",
      "300 0.0011088813189417124\n",
      "301 0.0010731718502938747\n",
      "302 0.0010386501671746373\n",
      "303 0.0010052899597212672\n",
      "304 0.0009730326710268855\n",
      "305 0.0009418043191544712\n",
      "306 0.0009116113651543856\n",
      "307 0.0008823981042951345\n",
      "308 0.0008541824063286185\n",
      "309 0.0008269167155958712\n",
      "310 0.0008005128474906087\n",
      "311 0.0007749749929644167\n",
      "312 0.0007502668886445463\n",
      "313 0.0007263556472025812\n",
      "314 0.0007032137946225703\n",
      "315 0.000680846453178674\n",
      "316 0.0006592218414880335\n",
      "317 0.0006382963038049638\n",
      "318 0.0006180473137646914\n",
      "319 0.0005984324961900711\n",
      "320 0.0005794566823169589\n",
      "321 0.000561124412342906\n",
      "322 0.0005433746264316142\n",
      "323 0.0005262124468572438\n",
      "324 0.0005096037057228386\n",
      "325 0.0004935211618430912\n",
      "326 0.0004779528535436839\n",
      "327 0.0004629049508366734\n",
      "328 0.00044834212167188525\n",
      "329 0.0004342415486462414\n",
      "330 0.0004206222656648606\n",
      "331 0.0004074003081768751\n",
      "332 0.00039461671258322895\n",
      "333 0.00038224330637604\n",
      "334 0.0003702579124365002\n",
      "335 0.00035866955295205116\n",
      "336 0.0003474430995993316\n",
      "337 0.00033657648600637913\n",
      "338 0.00032605562591925263\n",
      "339 0.00031587935518473387\n",
      "340 0.0003060227900277823\n",
      "341 0.00029648575582541525\n",
      "342 0.00028725244919769466\n",
      "343 0.00027830241015180945\n",
      "344 0.0002696506562642753\n",
      "345 0.0002612622920423746\n",
      "346 0.0002531467762310058\n",
      "347 0.00024530006339773536\n",
      "348 0.00023769134713802487\n",
      "349 0.00023033061006572098\n",
      "350 0.00022320004063658416\n",
      "351 0.00021629463299177587\n",
      "352 0.00020960000983905047\n",
      "353 0.00020312170090619475\n",
      "354 0.00019685090228449553\n",
      "355 0.0001907725672936067\n",
      "356 0.00018488873320166022\n",
      "357 0.00017918758385349065\n",
      "358 0.00017366581596434116\n",
      "359 0.0001683206355664879\n",
      "360 0.00016313944070134312\n",
      "361 0.00015812549099791795\n",
      "362 0.00015326710126828402\n",
      "363 0.00014855795598123223\n",
      "364 0.0001439975603716448\n",
      "365 0.00013958272757008672\n",
      "366 0.00013530605065170676\n",
      "367 0.00013117068738210946\n",
      "368 0.00012715980119537562\n",
      "369 0.0001232682407135144\n",
      "370 0.00011950283806072548\n",
      "371 0.00011585002357605845\n",
      "372 0.00011231807729927823\n",
      "373 0.00010888685937970877\n",
      "374 0.00010557377390796319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 0.00010235732042929158\n",
      "376 9.923728794092312e-05\n",
      "377 9.621901699574664e-05\n",
      "378 9.329617751063779e-05\n",
      "379 9.045679325936362e-05\n",
      "380 8.771116699790582e-05\n",
      "381 8.50461219670251e-05\n",
      "382 8.24676244519651e-05\n",
      "383 7.996294880285859e-05\n",
      "384 7.754081161692739e-05\n",
      "385 7.519596692873165e-05\n",
      "386 7.291958172572777e-05\n",
      "387 7.071298023220152e-05\n",
      "388 6.857894186396152e-05\n",
      "389 6.650610885117203e-05\n",
      "390 6.449886132031679e-05\n",
      "391 6.254936306504533e-05\n",
      "392 6.066340210963972e-05\n",
      "393 5.883549965801649e-05\n",
      "394 5.706298543373123e-05\n",
      "395 5.534719457500614e-05\n",
      "396 5.367810445022769e-05\n",
      "397 5.2066603529965505e-05\n",
      "398 5.049990795669146e-05\n",
      "399 4.898415500065312e-05\n",
      "400 4.751024971483275e-05\n",
      "401 4.608579911291599e-05\n",
      "402 4.470087151275948e-05\n",
      "403 4.3362771975807846e-05\n",
      "404 4.206318044452928e-05\n",
      "405 4.0802104194881395e-05\n",
      "406 3.958004526793957e-05\n",
      "407 3.839517739834264e-05\n",
      "408 3.724528505699709e-05\n",
      "409 3.6135879781795666e-05\n",
      "410 3.505598215269856e-05\n",
      "411 3.4008549846475944e-05\n",
      "412 3.299228410469368e-05\n",
      "413 3.201088111381978e-05\n",
      "414 3.1056220905156806e-05\n",
      "415 3.0129960578051396e-05\n",
      "416 2.923318425018806e-05\n",
      "417 2.836356907209847e-05\n",
      "418 2.7518886781763285e-05\n",
      "419 2.669938658073079e-05\n",
      "420 2.5907691451720893e-05\n",
      "421 2.513720937713515e-05\n",
      "422 2.438952105876524e-05\n",
      "423 2.3666545530431904e-05\n",
      "424 2.2963025912758894e-05\n",
      "425 2.2282883946900256e-05\n",
      "426 2.162350392609369e-05\n",
      "427 2.09820154850604e-05\n",
      "428 2.0358447727630846e-05\n",
      "429 1.975662053155247e-05\n",
      "430 1.9172906831954606e-05\n",
      "431 1.8604718206916004e-05\n",
      "432 1.8054974134429358e-05\n",
      "433 1.752395291987341e-05\n",
      "434 1.7007334463414736e-05\n",
      "435 1.6503674487466924e-05\n",
      "436 1.6017920643207617e-05\n",
      "437 1.5545860151178204e-05\n",
      "438 1.5086056919244584e-05\n",
      "439 1.4643519534729421e-05\n",
      "440 1.421302295057103e-05\n",
      "441 1.3793153812002856e-05\n",
      "442 1.3388158549787477e-05\n",
      "443 1.2994229109608568e-05\n",
      "444 1.2612033060577232e-05\n",
      "445 1.22406117952778e-05\n",
      "446 1.1882075341418386e-05\n",
      "447 1.1533147699083202e-05\n",
      "448 1.1194422768312506e-05\n",
      "449 1.0866179763979744e-05\n",
      "450 1.054783115250757e-05\n",
      "451 1.0238919458060991e-05\n",
      "452 9.938328730640933e-06\n",
      "453 9.646987564337905e-06\n",
      "454 9.365995538246352e-06\n",
      "455 9.09132450033212e-06\n",
      "456 8.825601980788633e-06\n",
      "457 8.567665645387024e-06\n",
      "458 8.316686944453977e-06\n",
      "459 8.074625839071814e-06\n",
      "460 7.838308192731347e-06\n",
      "461 7.609643944306299e-06\n",
      "462 7.387708592432318e-06\n",
      "463 7.172569439717336e-06\n",
      "464 6.963751275179675e-06\n",
      "465 6.75995852361666e-06\n",
      "466 6.563290753547335e-06\n",
      "467 6.37204539088998e-06\n",
      "468 6.187488907016814e-06\n",
      "469 6.006974672345677e-06\n",
      "470 5.83190285396995e-06\n",
      "471 5.662318926624721e-06\n",
      "472 5.4974138947727624e-06\n",
      "473 5.3380458666651975e-06\n",
      "474 5.1838133003911935e-06\n",
      "475 5.032815352024045e-06\n",
      "476 4.886756414634874e-06\n",
      "477 4.744358193420339e-06\n",
      "478 4.6069635573076084e-06\n",
      "479 4.472654836717993e-06\n",
      "480 4.344017725088634e-06\n",
      "481 4.217798959871288e-06\n",
      "482 4.095869826414855e-06\n",
      "483 3.977247615694068e-06\n",
      "484 3.862043286062544e-06\n",
      "485 3.7509466892515775e-06\n",
      "486 3.642056071839761e-06\n",
      "487 3.5365219446248375e-06\n",
      "488 3.434427526372019e-06\n",
      "489 3.3357991924276575e-06\n",
      "490 3.238766794311232e-06\n",
      "491 3.145286200378905e-06\n",
      "492 3.054230319321505e-06\n",
      "493 2.9664427074749256e-06\n",
      "494 2.8811934953409946e-06\n",
      "495 2.7973810574621893e-06\n",
      "496 2.7168432552571176e-06\n",
      "497 2.639339300003485e-06\n",
      "498 2.5631738935771864e-06\n",
      "499 2.489247663106653e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad= False)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_func(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Flow + Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T04:24:49.770250Z",
     "start_time": "2018-02-12T04:24:48.314163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 678.0689697265625\n",
      "1 698.9865112304688\n",
      "2 658.7703247070312\n",
      "3 643.0426635742188\n",
      "4 624.1532592773438\n",
      "5 667.052978515625\n",
      "6 586.9552001953125\n",
      "7 664.2916259765625\n",
      "8 654.0009765625\n",
      "9 541.649658203125\n",
      "10 482.1560363769531\n",
      "11 639.3900756835938\n",
      "12 503.0950622558594\n",
      "13 627.1452026367188\n",
      "14 382.1196594238281\n",
      "15 455.5534973144531\n",
      "16 600.8511962890625\n",
      "17 587.4323120117188\n",
      "18 632.5256958007812\n",
      "19 550.8008422851562\n",
      "20 527.6943969726562\n",
      "21 220.34628295898438\n",
      "22 572.947998046875\n",
      "23 548.740966796875\n",
      "24 433.2142028808594\n",
      "25 174.7963409423828\n",
      "26 158.20472717285156\n",
      "27 458.0625305175781\n",
      "28 115.33761596679688\n",
      "29 437.33477783203125\n",
      "30 293.7109375\n",
      "31 402.8432922363281\n",
      "32 383.5705261230469\n",
      "33 215.77601623535156\n",
      "34 345.1801452636719\n",
      "35 248.3778076171875\n",
      "36 131.49179077148438\n",
      "37 151.4760284423828\n",
      "38 272.351318359375\n",
      "39 240.56602478027344\n",
      "40 108.9260482788086\n",
      "41 95.68561553955078\n",
      "42 131.78335571289062\n",
      "43 115.88265991210938\n",
      "44 96.69530487060547\n",
      "45 286.2318115234375\n",
      "46 149.4499969482422\n",
      "47 271.1319274902344\n",
      "48 238.71556091308594\n",
      "49 144.5848846435547\n",
      "50 188.94711303710938\n",
      "51 191.24598693847656\n",
      "52 118.9694595336914\n",
      "53 254.45831298828125\n",
      "54 110.4941177368164\n",
      "55 109.12547302246094\n",
      "56 118.56951904296875\n",
      "57 168.8148651123047\n",
      "58 137.4752197265625\n",
      "59 114.60755920410156\n",
      "60 126.53893280029297\n",
      "61 93.59857940673828\n",
      "62 51.32950210571289\n",
      "63 282.9209899902344\n",
      "64 83.2723617553711\n",
      "65 165.98878479003906\n",
      "66 104.26718139648438\n",
      "67 85.37213134765625\n",
      "68 150.67489624023438\n",
      "69 183.23365783691406\n",
      "70 129.83172607421875\n",
      "71 94.42089080810547\n",
      "72 117.44747161865234\n",
      "73 131.9527130126953\n",
      "74 77.7431640625\n",
      "75 80.70789337158203\n",
      "76 76.1275405883789\n",
      "77 52.80305862426758\n",
      "78 53.65694808959961\n",
      "79 74.47759246826172\n",
      "80 55.69731140136719\n",
      "81 96.77474212646484\n",
      "82 28.59453773498535\n",
      "83 49.47820281982422\n",
      "84 29.354751586914062\n",
      "85 80.78288269042969\n",
      "86 62.298519134521484\n",
      "87 23.591581344604492\n",
      "88 65.52758026123047\n",
      "89 32.59664535522461\n",
      "90 30.58572006225586\n",
      "91 20.873815536499023\n",
      "92 99.81611633300781\n",
      "93 28.625642776489258\n",
      "94 34.5545768737793\n",
      "95 21.94734001159668\n",
      "96 50.57732009887695\n",
      "97 22.011362075805664\n",
      "98 38.63162612915039\n",
      "99 12.250877380371094\n",
      "100 13.713638305664062\n",
      "101 27.262113571166992\n",
      "102 28.088119506835938\n",
      "103 21.12921714782715\n",
      "104 13.13771915435791\n",
      "105 8.918536186218262\n",
      "106 9.392391204833984\n",
      "107 47.18461227416992\n",
      "108 34.930606842041016\n",
      "109 7.32409143447876\n",
      "110 7.909583568572998\n",
      "111 9.457418441772461\n",
      "112 34.9903678894043\n",
      "113 36.99292755126953\n",
      "114 12.310580253601074\n",
      "115 17.897836685180664\n",
      "116 29.370922088623047\n",
      "117 28.210315704345703\n",
      "118 23.896343231201172\n",
      "119 12.1810302734375\n",
      "120 8.88720417022705\n",
      "121 12.38735580444336\n",
      "122 17.776458740234375\n",
      "123 46.42362976074219\n",
      "124 20.197410583496094\n",
      "125 8.015020370483398\n",
      "126 10.500584602355957\n",
      "127 49.72734451293945\n",
      "128 7.949367523193359\n",
      "129 27.811662673950195\n",
      "130 23.29901695251465\n",
      "131 13.806119918823242\n",
      "132 20.699665069580078\n",
      "133 21.471792221069336\n",
      "134 9.112542152404785\n",
      "135 14.11695671081543\n",
      "136 11.297307014465332\n",
      "137 11.36197280883789\n",
      "138 9.842637062072754\n",
      "139 13.405050277709961\n",
      "140 5.992645263671875\n",
      "141 6.814348220825195\n",
      "142 5.069456100463867\n",
      "143 7.35693359375\n",
      "144 21.820919036865234\n",
      "145 9.146904945373535\n",
      "146 6.067479610443115\n",
      "147 11.888616561889648\n",
      "148 19.23990249633789\n",
      "149 6.444857597351074\n",
      "150 3.915700912475586\n",
      "151 13.211406707763672\n",
      "152 4.456661701202393\n",
      "153 8.729738235473633\n",
      "154 7.742555618286133\n",
      "155 6.5769195556640625\n",
      "156 4.493278503417969\n",
      "157 3.9043643474578857\n",
      "158 10.675802230834961\n",
      "159 3.705679416656494\n",
      "160 3.311033248901367\n",
      "161 3.646843671798706\n",
      "162 6.542391777038574\n",
      "163 4.276480197906494\n",
      "164 3.2127652168273926\n",
      "165 6.187890529632568\n",
      "166 5.174642086029053\n",
      "167 1.8890438079833984\n",
      "168 2.3471503257751465\n",
      "169 7.286674499511719\n",
      "170 2.3031656742095947\n",
      "171 3.4771018028259277\n",
      "172 1.3822693824768066\n",
      "173 3.6098296642303467\n",
      "174 3.3126919269561768\n",
      "175 2.6342763900756836\n",
      "176 1.2572431564331055\n",
      "177 1.4587488174438477\n",
      "178 1.5051301717758179\n",
      "179 1.3216466903686523\n",
      "180 2.577373743057251\n",
      "181 2.1621057987213135\n",
      "182 2.0483810901641846\n",
      "183 1.8902827501296997\n",
      "184 1.683772087097168\n",
      "185 5.15800666809082\n",
      "186 4.216254711151123\n",
      "187 0.9669973850250244\n",
      "188 1.500681757926941\n",
      "189 4.747819900512695\n",
      "190 1.5699888467788696\n",
      "191 0.8915302157402039\n",
      "192 0.8253781795501709\n",
      "193 0.7336571216583252\n",
      "194 3.1971261501312256\n",
      "195 0.47893190383911133\n",
      "196 1.7601898908615112\n",
      "197 1.4588700532913208\n",
      "198 0.5626741647720337\n",
      "199 0.6976951360702515\n",
      "200 1.8511451482772827\n",
      "201 2.8126466274261475\n",
      "202 1.0326309204101562\n",
      "203 1.4553672075271606\n",
      "204 0.6091145873069763\n",
      "205 0.5241004824638367\n",
      "206 0.41818204522132874\n",
      "207 1.3076448440551758\n",
      "208 4.524424076080322\n",
      "209 0.9595359563827515\n",
      "210 1.1698390245437622\n",
      "211 3.6961302757263184\n",
      "212 2.8773694038391113\n",
      "213 2.005617618560791\n",
      "214 0.9403702020645142\n",
      "215 0.4679441452026367\n",
      "216 3.906332492828369\n",
      "217 0.4065237045288086\n",
      "218 1.1063672304153442\n",
      "219 0.4403977394104004\n",
      "220 3.590961217880249\n",
      "221 2.2025954723358154\n",
      "222 2.4415414333343506\n",
      "223 3.6069047451019287\n",
      "224 3.1411240100860596\n",
      "225 1.732844591140747\n",
      "226 1.8255925178527832\n",
      "227 1.489169955253601\n",
      "228 0.3165517747402191\n",
      "229 3.825104236602783\n",
      "230 7.793625354766846\n",
      "231 1.2603724002838135\n",
      "232 3.3644094467163086\n",
      "233 7.1864800453186035\n",
      "234 1.761863350868225\n",
      "235 2.1055026054382324\n",
      "236 1.826936960220337\n",
      "237 3.1590099334716797\n",
      "238 3.5081374645233154\n",
      "239 3.2142672538757324\n",
      "240 2.9174273014068604\n",
      "241 3.908572196960449\n",
      "242 3.4119300842285156\n",
      "243 2.541142225265503\n",
      "244 2.2649333477020264\n",
      "245 4.11491060256958\n",
      "246 0.6840384602546692\n",
      "247 3.072497844696045\n",
      "248 0.5586124062538147\n",
      "249 1.5215519666671753\n",
      "250 1.492396354675293\n",
      "251 1.9389019012451172\n",
      "252 1.3274070024490356\n",
      "253 1.7484105825424194\n",
      "254 1.4087790250778198\n",
      "255 1.1428853273391724\n",
      "256 1.0305644273757935\n",
      "257 0.9969949126243591\n",
      "258 1.4487392902374268\n",
      "259 4.669349193572998\n",
      "260 1.9191993474960327\n",
      "261 1.4017256498336792\n",
      "262 5.350467205047607\n",
      "263 1.2585686445236206\n",
      "264 0.6217378973960876\n",
      "265 0.8268870711326599\n",
      "266 3.432689666748047\n",
      "267 1.9573583602905273\n",
      "268 0.7755863666534424\n",
      "269 3.0606958866119385\n",
      "270 1.3579041957855225\n",
      "271 2.6250927448272705\n",
      "272 0.861366868019104\n",
      "273 0.7872069478034973\n",
      "274 1.0799282789230347\n",
      "275 2.636711597442627\n",
      "276 0.9248913526535034\n",
      "277 0.4800332188606262\n",
      "278 1.514379620552063\n",
      "279 0.26765456795692444\n",
      "280 2.1236085891723633\n",
      "281 1.2523820400238037\n",
      "282 0.5676568150520325\n",
      "283 2.234715223312378\n",
      "284 0.45266595482826233\n",
      "285 0.38021206855773926\n",
      "286 0.2609938979148865\n",
      "287 0.8329381346702576\n",
      "288 0.12948286533355713\n",
      "289 1.6190580129623413\n",
      "290 1.8205251693725586\n",
      "291 0.3130050301551819\n",
      "292 0.4980807602405548\n",
      "293 2.904863119125366\n",
      "294 3.3275258541107178\n",
      "295 0.5987825393676758\n",
      "296 0.3308786153793335\n",
      "297 6.046870231628418\n",
      "298 0.3303925096988678\n",
      "299 0.13472099602222443\n",
      "300 1.2340154647827148\n",
      "301 2.641885280609131\n",
      "302 1.03040611743927\n",
      "303 0.14911337196826935\n",
      "304 1.523855209350586\n",
      "305 1.0325703620910645\n",
      "306 2.776611328125\n",
      "307 0.19295281171798706\n",
      "308 0.39931878447532654\n",
      "309 3.751450538635254\n",
      "310 1.2500807046890259\n",
      "311 0.6552799344062805\n",
      "312 0.09776751697063446\n",
      "313 0.24224399030208588\n",
      "314 4.550659656524658\n",
      "315 0.13902613520622253\n",
      "316 0.7395992875099182\n",
      "317 0.7124829888343811\n",
      "318 0.6059094667434692\n",
      "319 1.4360995292663574\n",
      "320 1.5264142751693726\n",
      "321 1.3356430530548096\n",
      "322 0.8833701610565186\n",
      "323 0.35538798570632935\n",
      "324 0.07082632929086685\n",
      "325 0.1591157764196396\n",
      "326 1.2782052755355835\n",
      "327 0.5999914407730103\n",
      "328 7.786457061767578\n",
      "329 0.69087815284729\n",
      "330 2.3393826484680176\n",
      "331 10.562835693359375\n",
      "332 1.821799874305725\n",
      "333 0.9356447458267212\n",
      "334 2.9664676189422607\n",
      "335 1.3407056331634521\n",
      "336 3.78399395942688\n",
      "337 0.8022739291191101\n",
      "338 1.5735222101211548\n",
      "339 2.941558361053467\n",
      "340 0.17267972230911255\n",
      "341 0.9664931297302246\n",
      "342 0.7717971801757812\n",
      "343 1.4120142459869385\n",
      "344 0.6978535056114197\n",
      "345 1.1735011339187622\n",
      "346 1.0825461149215698\n",
      "347 0.8791840672492981\n",
      "348 1.4254014492034912\n",
      "349 1.4477591514587402\n",
      "350 0.9427933692932129\n",
      "351 0.2724052667617798\n",
      "352 1.2597298622131348\n",
      "353 0.7697374820709229\n",
      "354 0.2185635268688202\n",
      "355 0.8073389530181885\n",
      "356 0.17914657294750214\n",
      "357 0.15453511476516724\n",
      "358 1.0284379720687866\n",
      "359 0.1483764797449112\n",
      "360 4.868966579437256\n",
      "361 0.5800347328186035\n",
      "362 1.9670031070709229\n",
      "363 2.1134138107299805\n",
      "364 1.8742481470108032\n",
      "365 0.5969256162643433\n",
      "366 0.4817022383213043\n",
      "367 1.5062239170074463\n",
      "368 1.3056139945983887\n",
      "369 2.115769147872925\n",
      "370 1.6921136379241943\n",
      "371 1.5595858097076416\n",
      "372 0.5347306728363037\n",
      "373 0.882427990436554\n",
      "374 1.2473905086517334\n",
      "375 1.2275043725967407\n",
      "376 0.5143857002258301\n",
      "377 0.4489854574203491\n",
      "378 0.5161309242248535\n",
      "379 0.5777335166931152\n",
      "380 0.5851117968559265\n",
      "381 0.5063202977180481\n",
      "382 0.38570159673690796\n",
      "383 0.3456915020942688\n",
      "384 0.40487533807754517\n",
      "385 0.7503645420074463\n",
      "386 0.20794768631458282\n",
      "387 0.3136362135410309\n",
      "388 0.5154010653495789\n",
      "389 0.2104538232088089\n",
      "390 0.16073308885097504\n",
      "391 0.12007405608892441\n",
      "392 0.4576655924320221\n",
      "393 0.10071839392185211\n",
      "394 0.26911211013793945\n",
      "395 0.3113211691379547\n",
      "396 0.29601824283599854\n",
      "397 1.475685715675354\n",
      "398 1.2594252824783325\n",
      "399 0.3622993230819702\n",
      "400 0.37467148900032043\n",
      "401 0.2540070116519928\n",
      "402 0.8272421360015869\n",
      "403 0.30349642038345337\n",
      "404 0.10581132024526596\n",
      "405 0.6398745179176331\n",
      "406 0.18231265246868134\n",
      "407 1.0900474786758423\n",
      "408 0.2988608777523041\n",
      "409 0.8687589168548584\n",
      "410 0.9092459082603455\n",
      "411 0.771389901638031\n",
      "412 0.6202826499938965\n",
      "413 0.691290020942688\n",
      "414 0.6210393309593201\n",
      "415 0.5041905045509338\n",
      "416 0.8183332681655884\n",
      "417 0.49079400300979614\n",
      "418 0.22431161999702454\n",
      "419 0.46757373213768005\n",
      "420 0.162057027220726\n",
      "421 0.12539324164390564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 0.5662491917610168\n",
      "423 0.4060453474521637\n",
      "424 0.7634985446929932\n",
      "425 0.6109599471092224\n",
      "426 0.47424450516700745\n",
      "427 0.35526612401008606\n",
      "428 0.4253063201904297\n",
      "429 0.6826678514480591\n",
      "430 0.09122802317142487\n",
      "431 0.8396509289741516\n",
      "432 0.4310610890388489\n",
      "433 0.7842603325843811\n",
      "434 0.09663848578929901\n",
      "435 0.5216723680496216\n",
      "436 0.0869370773434639\n",
      "437 0.0871482565999031\n",
      "438 0.7221881747245789\n",
      "439 0.6517196297645569\n",
      "440 0.3355465531349182\n",
      "441 0.5034245252609253\n",
      "442 0.35649916529655457\n",
      "443 0.455129474401474\n",
      "444 0.3244795799255371\n",
      "445 0.5439274311065674\n",
      "446 0.5268503427505493\n",
      "447 0.5181049108505249\n",
      "448 0.45050716400146484\n",
      "449 0.5589508414268494\n",
      "450 0.2538915276527405\n",
      "451 0.11221888661384583\n",
      "452 0.6219174861907959\n",
      "453 0.06911291927099228\n",
      "454 0.5454586744308472\n",
      "455 0.3072149157524109\n",
      "456 0.3765687048435211\n",
      "457 0.09236952662467957\n",
      "458 0.6934877038002014\n",
      "459 0.2841666042804718\n",
      "460 0.03727373480796814\n",
      "461 0.0409160852432251\n",
      "462 0.05244668200612068\n",
      "463 0.057924043387174606\n",
      "464 0.05032045766711235\n",
      "465 0.8779330253601074\n",
      "466 0.3622005879878998\n",
      "467 0.6951271295547485\n",
      "468 0.5113492012023926\n",
      "469 1.1251140832901\n",
      "470 0.5875971913337708\n",
      "471 0.5978733897209167\n",
      "472 0.054899659007787704\n",
      "473 1.2559599876403809\n",
      "474 0.6165726780891418\n",
      "475 0.04812871664762497\n",
      "476 0.32238247990608215\n",
      "477 0.3236764669418335\n",
      "478 0.38049596548080444\n",
      "479 0.3826816976070404\n",
      "480 0.289969801902771\n",
      "481 0.21549274027347565\n",
      "482 0.22014974057674408\n",
      "483 0.3153389096260071\n",
      "484 0.24460290372371674\n",
      "485 0.32732877135276794\n",
      "486 1.9047493934631348\n",
      "487 0.16924433410167694\n",
      "488 0.9511564373970032\n",
      "489 0.48014068603515625\n",
      "490 1.179181456565857\n",
      "491 0.39926034212112427\n",
      "492 0.7337572574615479\n",
      "493 0.1264757364988327\n",
      "494 0.1308755725622177\n",
      "495 1.412621021270752\n",
      "496 0.08318190276622772\n",
      "497 0.5435693860054016\n",
      "498 0.07682367414236069\n",
      "499 0.08488571643829346\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
